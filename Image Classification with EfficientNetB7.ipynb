{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install git+https://github.com/qubvel/efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.preprocessing import image\n",
    "from efficientnet.keras import EfficientNetB7\n",
    "import os\n",
    "import re\n",
    "\n",
    "from keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = ''\n",
    "train_folders_path = os.path.join(base_path,'train')\n",
    "validation_folders_path = ''\n",
    "test_folders_path = os.path.join(base_path,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classlabels = os.listdir(train_folders_path)\n",
    "print(classlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimensions\n",
    "img_width, img_height = 224,224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENB7Model = EfficientNetB7(include_top=False, input_shape= (img_width,img_height,3))\n",
    "ENB7Model.trainable = False       #Freeze the weights of the transferred model\n",
    "ENB7Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictionLayer = Dense(units = len(classlabels), activation = 'softmax')\n",
    "model = Sequential([ENB7Model,   \n",
    "                    Dropout(0.3),                           # Prevent overfitting by randomly dropping units with a probability of 30%\n",
    "                    GlobalAveragePooling2D(),\n",
    "                    PredictionLayer])\n",
    "\n",
    "\n",
    "model.compile(optimizer = keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   rotation_range=30,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True,\n",
    "                                   validation_split=0.2)\n",
    "\n",
    "print('Making training data generator...')\n",
    "training_set = datagen.flow_from_directory(train_folders_path,\n",
    "                                           target_size = (img_width, img_height),\n",
    "                                           batch_size = 32,\n",
    "                                           class_mode = 'categorical',\n",
    "                                           subset='training')\n",
    "\n",
    "print('Making validation data generator...')\n",
    "cv_set = datagen.flow_from_directory(train_folders_path,\n",
    "                                       target_size = (img_width, img_height),\n",
    "                                       batch_size = 32,\n",
    "                                       class_mode = 'categorical',\n",
    "                                       subset='validation',\n",
    "                                       shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES1 = EarlyStopping(monitor='val_accuracy', patience=20)\n",
    "MC1 = ModelCheckpoint(\n",
    "    filepath=(os.path.join(base_path, 'EfficientNetB7 with top layers warmed.h5')),\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1)\n",
    "LR1 = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=5, min_lr = 1e-8, mode='max',verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Warming up the top layers')\n",
    "history = model.fit_generator(training_set,\n",
    "                         steps_per_epoch = (training_set.n // 32)+1,\n",
    "                         epochs = 100,\n",
    "                         validation_data = cv_set,\n",
    "                         validation_steps = (cv_set.n//32)+1,\n",
    "                         callbacks=[ES1,MC1,LR1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES2 = EarlyStopping(monitor='val_accuracy', patience=20)\n",
    "MC2 = ModelCheckpoint(\n",
    "    filepath=(os.path.join(base_path, 'EfficientNetB7 Full.h5')),\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1)\n",
    "LR2 = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=5, min_lr = 1e-8, mode='max',verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENB7Model.trainable = True            # Unfreeze the pre trained weights\n",
    "#for layer in ENB7Model.layers[:x]:    where x is an integer representing the layer you want to freeze it up till. ignore if you want to fine tune the whole model\n",
    "#    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer = keras.optimizers.Adam(learning_rate=0.0001),    # slower learning rate to not update the pre trained weights too much\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit_generator(training_set,\n",
    "                         steps_per_epoch = (training_set.n // 32)+1,\n",
    "                         epochs = 200,\n",
    "                         initial_epoch =  history.epoch[-1],\n",
    "                         validation_data = cv_set,\n",
    "                         validation_steps = (cv_set.n//32)+1,\n",
    "                         callbacks=[ES2,MC2,LR2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history2.history['accuracy']\n",
    "val_acc += history2.history['val_accuracy']\n",
    "\n",
    "loss += history2.history['loss']\n",
    "val_loss += history2.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For predicting with a csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testimages_list=[]\n",
    "for testimages_name in test['input column name here']:\n",
    "    testimages_path = os.path.join(test_folders_path,testimages_name)\n",
    "    testimages_list.append(testimages_path)\n",
    "testimages_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "testresult_list = []\n",
    "for img in tqdm(range(len(testimages_list))):\n",
    "    test_image = image.load_img(testimages_list[img],target_size = (img_width, img_height))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = test_image / 255.\n",
    "    test_image = np.expand_dims(test_image,axis=0)\n",
    "    result = model.predict(test_image)\n",
    "    highest_probability = int(np.argmax(result[0]))\n",
    "    predicted_class = classlabels[highest_probability]\n",
    "    test_result = [testimages_list[img],predicted_class]\n",
    "    testresult_list.append(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df = pd.DataFrame(testresult_list,columns=['Image','category'])\n",
    "test['category']=predicted_df['category']\n",
    "test['category']=test.category.apply(lambda c:str(c).zfill(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(os.path.join(base_path,'test_final.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
